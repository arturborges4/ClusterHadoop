# ClusterHadoop no HyperVisor Proxmox
Cluster Hadoop utilizando VMs Ubuntu Server no Proxmox.


üñ•Ô∏èConfigura√ß√£o do Cluster

Foi utilizado o HyperVisor Proxmox rodando em um servidor para provisionar maquinas virtuais (VMs), instalado o Ubuntu Server na sua vers√£o 20.04 (afim de economizar recursos, n√£o foi usada a ultima vers√£o do Ubuntu), o Java JDK 8 e o Hadoop 3.3.6. 
Configura√ß√£o da maquina Host:
 - Processador FX8300 8 Cores 3.3 GHz base core.
 - Mem√≥ria RAM 3.6 GBs DDR3.
 - HDD 1TB.

![Servidor_resized](https://github.com/user-attachments/assets/0bdc4c49-3a56-4c06-ad3a-360547294418)

Definidos os IPs est√°ticos para as m√°quinas diretamente nas configura√ß√µes do roteador.

![IPSestaticosROTEADOR_resized_resized](https://github.com/user-attachments/assets/406b2732-d98d-4a63-98d3-5b665c874536)


Ap√≥s as configura√ß√µes, o n√≥ Master passou a identificar os n√≥s Slaves.

![datanodes3](https://github.com/user-attachments/assets/151d16f0-bd84-46f2-9315-52ab14f83218)

![datanodesWEBGUI](https://github.com/user-attachments/assets/2813e648-d57d-4707-846a-0b39dc9ace06)

Na interface WEB do Proxmox foi possivel acompanhar o consumo de recursos do servidor. Destaque para a limita√ß√£o de mem√≥ria com as tr√™s m√°quinas rodando simultaneamente.

![proxmox](https://github.com/user-attachments/assets/919f4899-9212-41c7-a7b1-92fd8ad29e03)


<h1>1. Fun√ß√£o de cada n√≥ no cluster Hadoop:</h1>

<h3>NameNode (Master):</h3>

√â respons√°vel por gerenciar o metadata do sistema de arquivos distribu√≠do HDFS (ex.: nomes dos arquivos, blocos, localiza√ß√µes nos DataNodes).
Coordena as opera√ß√µes de leitura/escrita e √© essencial para a opera√ß√£o do cluster.

<h3>DataNode (Slaves):</h3>

Armazenam os dados reais em blocos.
Respondem √†s solicita√ß√µes do NameNode para leitura, escrita e replica√ß√£o de dados.

<h3>ResourceManager (Master):</h3>

Gerencia os recursos do cluster para execu√ß√£o de aplica√ß√µes distribu√≠das.
Trabalha com o NodeManager para alocar cont√™ineres e monitorar tarefas.

<h3>NodeManager (Slaves):</h3>

Executa tarefas atribu√≠das pelo ResourceManager.
Monitora o uso de recursos (CPU, mem√≥ria) em cada n√≥.



<h1>2. Configura√ß√µes alteradas nos arquivos do Hadoop e motivos:</h1>

<h3>core-site.xml</h3>
fs.defaultFS: Configurado com o endere√ßo do NameNode (hdfs://192.168.100.193:9000) para centralizar a comunica√ß√£o.
Motivo: Informar o local do sistema de arquivos HDFS para todas as opera√ß√µes.

<h3>hdfs-site.xml</h3>
dfs.namenode.name.dir e dfs.datanode.data.dir: Diret√≥rios locais configurados para armazenar os dados do NameNode e DataNodes.
dfs.replication: Definido como 3, para garantir redund√¢ncia dos dados.
Motivo: Configurar o armazenamento f√≠sico e a replica√ß√£o dos dados no cluster.

<h3>yarn-site.xml</h3>
yarn.resourcemanager.hostname: Configurado com o endere√ßo do ResourceManager (master).
yarn.nodemanager.aux-services: Ativou o servi√ßo de shuffle necess√°rio para a execu√ß√£o de jobs no YARN.
Motivo: Permitir que o YARN coordene a execu√ß√£o das aplica√ß√µes distribu√≠das.

<h3>slaves</h3>
Adicionados os endere√ßos dos n√≥s slave (slave1 e slave2).
Motivo: Informar ao cluster os n√≥s que devem atuar como DataNodes e NodeManagers.



<h1>Execu√ß√£o da Aplica√ß√£o</h1>
<h3>1. Etapas para executar o WordCount:</h3>

- Criado um arquivo input.txt contendo palavras simples.
- Usado o comando ```hadoop fs -mkdir``` para criar um diret√≥rio de entrada no HDFS.
- Carregado o arquivo input.txt no HDFS com ```hadoop fs -put```
- Executado o job WordCount usando:
```hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /user/hadoop/input /user/hadoop/output```
- Verifica√ß√£o dos resultados com ```hadoop fs -cat /user/hadoop/output/part-r-00000```


<h3>2. Onde o arquivo foi enviado e como foi especificado:</h3>
O arquivo input.txt foi enviado ao HDFS no diret√≥rio /user/hadoop/input.
Na execu√ß√£o do WordCount, o diret√≥rio de entrada foi especificado como /user/hadoop/input, e o de sa√≠da foi definido como /user/hadoop/output.

<h3>3. Como o Hadoop divide e distribui as tarefas:</h3>

- O HDFS divide o arquivo de entrada em blocos (padr√£o: 128 MB).

- Cada bloco √© processado por um Map Task, atribu√≠do aos DataNodes que armazenam os blocos.

- O Reduce Task combina os resultados intermedi√°rios dos Map Tasks, produzindo a sa√≠da final.



<h1>An√°lise de Desempenho</h1>

<h3>1. Tempo para processar e fatores que influenciam o desempenho: </h3>

- O processamento foi prejudicado pelas limita√ß√µes de Hardware da m√°quina host, que contava com apenas um processador AMD fx8300 de 8 cores, 3.6 GBs DDR3 de mem√≥ria RAM e um disco r√≠gido HDD. A baixa disponibilidade de mem√≥ria foi um fator crucial, pois gerou a necessidade de alocar apenas 1GB e 2 cores para cada n√≥ do cluster, j√° que o pr√≥prio HyperVisor Proxmox necessita de recursos para trabalhar. 


<h3>2. Distribui√ß√£o da carga entre os n√≥s:</h3>

- O job foi distribu√≠do entre os tr√™s DataNodes, com cada n√≥ processando blocos armazenados localmente.

- Raz√µes para carga desigual:

     - Dados locais: O Hadoop tenta processar blocos no mesmo n√≥ onde est√£o armazenados.
     
     - N√∫mero de blocos: Se o arquivo possui poucos blocos, nem todos os n√≥s ser√£o usados igualmente.    

     - Configura√ß√£o do cluster: Par√¢metros como n√∫mero de Map/Reduce Tasks podem limitar a paraleliza√ß√£o.



<h1>Desafios e Resolu√ß√£o de Problemas</h1>

<h3>1. Desafios enfrentados e solu√ß√µes aplicadas:</h3>

- Desafio 1: Instala√ß√£o das vers√µes corretas.
     - O primeiro problema enfrentado foi a instala√ß√£o do Java JDK. A Hadoop √© projetado para trabalhar com a vers√£o 8 do Java na build escolhida (3.3.6). Ao tentar rodar com o Java 11, problemas de compatibilidade dificultaram a configura√ß√£o. O reset da VM para    
     instala√ß√£o do Java 8 permitiu prosseguir.


- Desafio 2: Problemas de conectividade entre os n√≥s.
     - Sintoma: Os slaves n√£o eram reconhecidos pelo master, resultando em falhas na execu√ß√£o de jobs.
     - Causa: Endere√ßos IP din√¢micos atribu√≠dos √†s VMs alteravam a comunica√ß√£o entre os n√≥s.
     - Solu√ß√£o: Configurado IPs est√°ticos para todas as VMs e atualizado o arquivo hosts para associar nomes e IPs.



- Desafio 3: Diret√≥rios de armazenamento ausentes.
     - Sintoma: Erro ao inicializar o NameNode/DataNodes devido √† falta dos diret√≥rios configurados.
     - Causa: Diret√≥rios especificados nos arquivos de configura√ß√£o n√£o haviam sido criados manualmente.
     - Solu√ß√£o: Criei os diret√≥rios especificados em dfs.namenode.name.dir e dfs.datanode.data.dir e ajustei permiss√µes com chown.
 

- Desafio 4: Erros de permiss√µes no HDFS.
     - Sintoma: Impossibilidade de carregar arquivos para o HDFS devido a erros de acesso.
     - Causa: O usu√°rio do sistema n√£o tinha permiss√µes adequadas para operar no HDFS.
     - Solu√ß√£o: Ajustei permiss√µes usando hdfs dfs -chmod e configurei o ambiente para que o usu√°rio artur fosse o propriet√°rio do HDFS.
 

- Desafio 5: Lerdeza na execu√ß√£o de jobs.
     - Sintoma: Jobs simples demoravam mais do que o esperado.
     - Causa: Recursos insuficientes (mem√≥ria e CPU) dispon√≠veis nas VMs.
     - Solu√ß√£o: Rdistribuido os recursos alocados no Proxmox, garantindo 1 GB por n√≥ e otimizado o tamanho do arquivo de entrada para testes.

 
- Desafio 6: Arquivo de configura√ß√£o inv√°lido.
     - Sintoma: Erro ao iniciar o cluster devido a configura√ß√µes incorretas nos arquivos XML.
     - Causa: Sintaxe inv√°lida (ex.: tags n√£o fechadas) ou valores inconsistentes.
     - Solu√ß√£o: Valida√ß√£o dos arquivos com aten√ß√£o √† sintaxe e utilizados exemplos de configura√ß√£o como refer√™ncia.



<h3>2. Problemas comuns na configura√ß√£o de um cluster Hadoop e como resolv√™-los:</h3>
- Problema 1: Comunica√ß√£o entre n√≥s.
     - Causa: DNS mal configurado ou falta de IPs fixos.
     - Dica: Configure o arquivo /etc/hosts para associar nomes e IPs de todos os n√≥s. Use IPs est√°ticos para evitar inconsist√™ncias.


- Problema 2: Diret√≥rios do HDFS ausentes ou incorretos.
     - Causa: Diret√≥rios especificados nos arquivos de configura√ß√£o n√£o existem.
     - Dica: Crie manualmente os diret√≥rios configurados em hdfs-site.xml e defina permiss√µes corretas.

 
- Problema 3: Erros ao iniciar o YARN.
     - Causa: Falta de par√¢metros necess√°rios no yarn-site.xml.
     - Dica: Verifique se ```yarn.resourcemanager.hostname``` e ```yarn.nodemanager.aux-services``` est√£o configurados corretamente.
 
- Problema 4: Falha ao carregar dados no HDFS.
     - Causa: Usu√°rio do sistema n√£o tem permiss√µes adequadas.
     - Dica: Certifique-se de que o usu√°rio correto seja propriet√°rio do HDFS e configure permiss√µes adequadas com hdfs dfs -chmod.
 
- Problema 5: Jobs MapReduce n√£o distribu√≠dos.
     - Causa: Arquivo de entrada pequeno ou configura√ß√£o de tarefas limitada.
     - Dica: Use arquivos maiores para testar a distribui√ß√£o e ajuste os par√¢metros de Map/Reduce Tasks.
 

LOGS WORDCOUNT:

![wordcount-teste-3nos](https://github.com/user-attachments/assets/47e45756-a355-4c62-9263-d98b53f4d022)
